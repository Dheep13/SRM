{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a print-friendly version of the Machine Learning Cheat Sheet:\n",
    "\n",
    "## Machine Learning Cheat Sheet\n",
    "\n",
    "### 1. Key ML Algorithms\n",
    "\n",
    "**Supervised Learning - Classification**\n",
    "\n",
    "| Algorithm | Use Cases | Key Parameters | Pros | Cons |\n",
    "|-----------|-----------|----------------|------|------|\n",
    "| Logistic Regression | Binary classification, Spam detection, Risk assessment | C, penalty, solver | Simple & fast, Interpretable, Good for linear data | Can't handle non-linear data, Assumes independence |\n",
    "| Decision Trees | Multi-class classification, Feature importance, Non-linear data | max_depth, min_samples_split, criterion | Easy to visualize, Handles non-linear data, No scaling needed | Can overfit, Unstable, Biased to dominant classes |\n",
    "| Random Forest | Complex classification, Ensemble learning, Feature selection | n_estimators, max_features, bootstrap | Reduces overfitting, Handles missing values, Feature importance | Black box model, Computationally heavy |\n",
    "| SVM | High-dimensional data, Text classification, Image recognition | kernel, C, gamma | Works in high dimensions, Memory efficient, Versatile kernels | Sensitive to scaling, Slow training, Hard to interpret |\n",
    "\n",
    "**Supervised Learning - Regression**\n",
    "\n",
    "| Algorithm | Use Cases | Key Parameters | Pros | Cons |\n",
    "|-----------|-----------|----------------|------|------|\n",
    "| Linear Regression | Simple prediction, Baseline model, Feature importance | fit_intercept, normalize, n_jobs | Simple & interpretable, Fast training, Feature importance | Assumes linearity, Sensitive to outliers |\n",
    "| Ridge (L2) | Multicollinearity, Continuous prediction, Feature selection | alpha, solver, normalize | Handles multicollinearity, Reduces overfitting, Stable solutions | Assumes linearity, Keeps all features |\n",
    "| Lasso (L1) | Sparse solutions, Feature selection, Automated selection | alpha, selection, normalize | Feature selection, Sparse solutions, Handles high dimensions | Unstable with correlated features, Needs tuning |\n",
    "\n",
    "**Unsupervised Learning**\n",
    "\n",
    "| Algorithm | Use Cases | Key Parameters | Pros | Cons |\n",
    "|-----------|-----------|----------------|------|------|\n",
    "| K-Means | Clustering, Segmentation, Grouping | n_clusters, init, n_init | Simple & fast, Scalable, Easy to understand | Needs k value, Sensitive to outliers |\n",
    "| DBSCAN | Density clustering, Noise detection, Variable shapes | eps, min_samples, metric | Finds any shape, Handles noise, No preset clusters | Sensitive to parameters, Struggles with varying densities |\n",
    "| PCA | Dimension reduction, Feature extraction, Visualization | n_components, svd_solver, whiten | Reduces dimensions, Handles multicollinearity, Unsupervised | Linear assumptions, Loss of interpretability |\n",
    "\n",
    "### 2. Evaluation Metrics\n",
    "\n",
    "**Classification Metrics**\n",
    "\n",
    "| Metric | Formula | When to Use | Implementation |\n",
    "|--------|---------|-------------|----------------|\n",
    "| Accuracy | (TP + TN)/(TP + TN + FP + FN) | Balanced datasets | metrics.accuracy_score() |\n",
    "| Precision | TP/(TP + FP) | Minimize false positives | metrics.precision_score() |\n",
    "| Recall | TP/(TP + FN) | Minimize false negatives | metrics.recall_score() |\n",
    "| F1 Score | 2×(P×R)/(P + R) | Balance precision/recall | metrics.f1_score() |\n",
    "| ROC-AUC | Area under ROC curve | Binary classification | metrics.roc_auc_score() |\n",
    "\n",
    "**Regression Metrics**\n",
    "\n",
    "| Metric | Formula | When to Use | Implementation |\n",
    "|--------|---------|-------------|----------------|\n",
    "| MSE | Σ(y_true - y_pred)²/n | General purpose | metrics.mean_squared_error() |\n",
    "| RMSE | √(MSE) | Same units as target | np.sqrt(metrics.mean_squared_error()) |\n",
    "| MAE | Σ\\|y_true - y_pred\\|/n | Robust to outliers | metrics.mean_absolute_error() |\n",
    "| R² | 1 - (MSE/Var(y)) | Model fit quality | metrics.r2_score() |\n",
    "\n",
    "### 3. Essential Python Code Snippets\n",
    "\n",
    "**Data Loading & Preprocessing**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.dropna(inplace=True)\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**Model Training & Evaluation**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "```\n",
    "\n",
    "**Hyperparameter Tuning**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params = {'C': [0.1, 1, 10], 'kernel': ['rbf', 'linear']}\n",
    "grid = GridSearchCV(model, params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "```\n",
    "\n",
    "### 4. Feature Engineering Techniques\n",
    "\n",
    "| Technique | Purpose | Implementation |\n",
    "|-----------|---------|----------------|\n",
    "| Scaling | Normalize features | StandardScaler(), MinMaxScaler() |\n",
    "| Encoding | Handle categories | LabelEncoder(), OneHotEncoder() |\n",
    "| Selection | Reduce dimensions | SelectKBest(), RFE() |\n",
    "| Creation | Make new features | PolynomialFeatures() |\n",
    "| Binning | Group continuous data | pd.cut(), pd.qcut() |\n",
    "\n",
    "### 5. Common Errors & Solutions\n",
    "\n",
    "| Problem | Symptoms | Solutions |\n",
    "|---------|----------|-----------|\n",
    "| Overfitting | High train score, Low test score | More data, Regularization, Reduce complexity |\n",
    "| Underfitting | Low train score, Low test score | More features, Less regularization, More complex model |\n",
    "| Data Leakage | Unrealistic high scores | Proper CV splits, Feature scaling after split |\n",
    "| Class Imbalance | High accuracy, low recall | SMOTE, Class weights, Stratification |\n",
    "\n",
    "### 6. Best Practices\n",
    "\n",
    "1. Data Preprocessing:\n",
    "   - Handle missing values first\n",
    "   - Scale features appropriately\n",
    "   - Check for class imbalance\n",
    "   - Split data before scaling\n",
    "\n",
    "2. Model Selection:\n",
    "   - Start simple\n",
    "   - Use cross-validation\n",
    "   - Consider computational cost\n",
    "   - Check assumptions\n",
    "\n",
    "3. Model Evaluation:\n",
    "   - Use multiple metrics\n",
    "   - Check for overfitting\n",
    "   - Consider business impact\n",
    "   - Validate on holdout set\n",
    "\n",
    "4. Production:\n",
    "   - Save preprocessing steps\n",
    "   - Version control models\n",
    "   - Monitor performance\n",
    "   - Plan for updates\n",
    "\n",
    "### 7. Key Libraries\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import (\n",
    "    preprocessing,\n",
    "    model_selection,\n",
    "    metrics,\n",
    "    ensemble,\n",
    "    linear_model,\n",
    "    svm,\n",
    "    tree\n",
    ")\n",
    "```\n",
    "\n",
    "**Remember:**\n",
    "- Start with simple models\n",
    "- Always split data properly\n",
    "- Use cross-validation\n",
    "- Check assumptions\n",
    "- Document everything\n",
    "- Monitor performance\n",
    "\n",
    "Citations:\n",
    "[1] https://ppl-ai-file-upload.s3.amazonaws.com/web/direct-files/15813152/d6066d59-2144-4ba4-acb3-5816e9292679/paste.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're right. Let me reformat the SVM cheat sheet with proper table formatting where all content stays within columns. Here's the corrected version:\n",
    "\n",
    "# Support Vector Machine (SVM) Cheat Sheet\n",
    "\n",
    "## 1. Types of SVM\n",
    "\n",
    "| Type | Description | Use Cases | Key Parameters |\n",
    "|------|-------------|-----------|----------------|\n",
    "| Linear SVM | Uses linear hyperplane for separation; Maximizes margin between classes | Text classification; High dimensional data; Linear separable data | C: regularization strength; max_iter: iterations; tol: tolerance |\n",
    "| Non-linear SVM | Uses kernel trick; Transforms data to higher dimensions | Image classification; Complex patterns; Non-linear data | kernel: kernel type; C: regularization; gamma: coefficient |\n",
    "| SVM Regression | Predicts continuous values; Uses epsilon-tube | Price prediction; Time series; Continuous data | epsilon: margin width; C: regularization; kernel: type |\n",
    "\n",
    "## 2. Kernel Types\n",
    "\n",
    "| Kernel | Formula | Use Case | Parameters |\n",
    "|--------|---------|----------|------------|\n",
    "| Linear | K(x,y) = x^T y | High dimensional data; Text classification; Simple datasets | None needed |\n",
    "| RBF (Gaussian) | K(x,y) = exp(-γ‖x-y‖²) | Non-linear data; Image processing; General purpose | gamma: kernel coefficient |\n",
    "| Polynomial | K(x,y) = (γx^T y + r)^d | Image processing; Natural language; Feature interactions | degree: polynomial degree; gamma: scale; coef0: constant |\n",
    "| Sigmoid | K(x,y) = tanh(γx^T y + r) | Neural network alternative; Binary classification | gamma: scale; coef0: constant |\n",
    "\n",
    "## 3. Important Parameters\n",
    "\n",
    "| Parameter | Purpose | Typical Values | Effect |\n",
    "|-----------|---------|----------------|---------|\n",
    "| C | Controls regularization strength | 0.1 to 100; Default: 1.0 | Large C: Less regularization; Small C: More regularization |\n",
    "| gamma | Controls influence range | scale, auto, 0.001 to 1 | Large: Close influence; Small: Far influence |\n",
    "| kernel | Defines transformation type | rbf, linear, poly, sigmoid | Changes data transformation; Affects complexity |\n",
    "| degree | Sets polynomial complexity | 2 to 5; Default: 3 | Higher: More complex; Lower: Simpler |\n",
    "\n",
    "## 4. Advantages and Disadvantages\n",
    "\n",
    "| Advantages | Disadvantages |\n",
    "|------------|---------------|\n",
    "| Effective in high dimensions; Memory efficient; Versatile kernels; Robust to overfitting | Sensitive to scaling; Slow on large datasets; Needs parameter tuning; Black box model |\n",
    "\n",
    "## 5. Best Practices\n",
    "\n",
    "| Area | Recommendations |\n",
    "|------|-----------------|\n",
    "| Data Preparation | Scale all features; Handle missing values; Remove outliers; Convert categorical data |\n",
    "| Kernel Selection | Start linear; Try RBF for non-linear; Use polynomial for interactions; Cross-validate |\n",
    "| Parameter Tuning | Grid search C and gamma; Start with defaults; Use logarithmic scales; Monitor time |\n",
    "| Optimization | Use approximation for large data; Consider feature selection; Monitor support vectors |\n",
    "\n",
    "## 6. Common Issues and Solutions\n",
    "\n",
    "| Issue | Symptoms | Solutions |\n",
    "|-------|----------|-----------|\n",
    "| Overfitting | High train, low test scores | Decrease C; Use simpler kernel; Add regularization |\n",
    "| Underfitting | Low train and test scores | Increase C; Try different kernel; Add features |\n",
    "| Slow Training | Long convergence time | Use linear kernel; Reduce dataset; Select features |\n",
    "| Poor Performance | Low accuracy, unstable | Scale features; Change kernel; Tune parameters |\n",
    "\n",
    "## 7. Basic Implementation\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Basic SVM\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale')\n",
    "svm.fit(X_scaled, y)\n",
    "\n",
    "# Grid Search\n",
    "params = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 'auto', 0.1],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "grid_search = GridSearchCV(SVC(), params, cv=5)\n",
    "```\n",
    "\n",
    "Key Points to Remember:\n",
    "1. Always scale features\n",
    "2. Start with simple models\n",
    "3. Use cross-validation\n",
    "4. Monitor training time\n",
    "5. Check support vectors\n",
    "\n",
    "Would you like me to adjust the formatting of any particular section or add more details to any topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Naive Bayes Cheat Sheet\n",
    "\n",
    "## 1. Basic Concepts\n",
    "\n",
    "| Term | Symbol | Meaning |\n",
    "|------|--------|---------|\n",
    "| Posterior | P(A\\|B) | Probability of A given B |\n",
    "| Likelihood | P(B\\|A) | Probability of B given A |\n",
    "| Prior | P(A) | Initial probability of A |\n",
    "| Product | ∏ | Multiply sequence of terms |\n",
    "| Mean | μ | Average of distribution |\n",
    "| Std Dev | σ | Spread of distribution |\n",
    "\n",
    "## 2. Types of Naive Bayes\n",
    "\n",
    "| Type | Data Type | Best For | Example Data |\n",
    "|------|-----------|----------|--------------|\n",
    "| Gaussian | Continuous | Physical measurements | Height: 175.5 cm |\n",
    "| Multinomial | Count data | Text classification | Word appears 3 times |\n",
    "| Bernoulli | Binary data | Presence/absence | Word exists: yes/no |\n",
    "\n",
    "## 3. Formulas and Smoothing\n",
    "\n",
    "### A. Gaussian NB\n",
    "```\n",
    "P(x|class) = 1/(√(2πσ²)) × e^(-(x-μ)²/2σ²)\n",
    "\n",
    "Where:\n",
    "x = feature value\n",
    "μ = class mean\n",
    "σ = class standard deviation\n",
    "```\n",
    "\n",
    "### B. Multinomial NB\n",
    "```\n",
    "With vocabulary smoothing:\n",
    "P(word|class) = (count + α)/(total + α|V|)\n",
    "\n",
    "Where:\n",
    "count = word occurrences in class\n",
    "total = all words in class\n",
    "|V| = vocabulary size\n",
    "α = smoothing parameter\n",
    "```\n",
    "\n",
    "### C. Bernoulli NB\n",
    "```\n",
    "With class smoothing:\n",
    "P(word|class) = (count + α)/(total + αk)\n",
    "\n",
    "Where:\n",
    "count = documents with word in class\n",
    "total = documents in class\n",
    "k = number of classes\n",
    "α = smoothing parameter\n",
    "```\n",
    "\n",
    "## 4. Practical Examples\n",
    "\n",
    "### A. Gaussian Example (Height Classification)\n",
    "```\n",
    "Given:\n",
    "Male: μ = 175cm, σ = 10\n",
    "Female: μ = 162cm, σ = 8\n",
    "New height = 168cm\n",
    "\n",
    "P(height|male) = 1/(√(2π×10²)) × e^(-(168-175)²/(2×10²))\n",
    "                = 0.0312\n",
    "\n",
    "P(height|female) = 1/(√(2π×8²)) × e^(-(168-162)²/(2×8²))\n",
    "                 = 0.0376\n",
    "\n",
    "Result: Classify as Female (0.0376 > 0.0312)\n",
    "```\n",
    "\n",
    "### B. Multinomial Example (Text Classification)\n",
    "```\n",
    "Given:\n",
    "- Word 'money' appears 20 times in spam\n",
    "- Total spam emails: 100\n",
    "- Vocabulary size: 1000\n",
    "- α = 1\n",
    "\n",
    "P(money|spam) = (20 + 1)/(100 + 1×1000)\n",
    "               = 21/1100\n",
    "               ≈ 0.019\n",
    "```\n",
    "\n",
    "### C. Bernoulli Example (Spam Detection)\n",
    "```\n",
    "Given:\n",
    "- Word 'money' appears in 20 spam emails\n",
    "- Total spam emails: 100\n",
    "- Number of classes: 2\n",
    "- α = 1\n",
    "\n",
    "P(money|spam) = (20 + 1)/(100 + 1×2)\n",
    "               = 21/102\n",
    "               ≈ 0.206\n",
    "```\n",
    "\n",
    "## 5. Implementation in Python\n",
    "\n",
    "```python\n",
    "# Gaussian NB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Multinomial NB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB(alpha=1.0)\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Bernoulli NB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB(alpha=1.0)\n",
    "bnb.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "## 6. When to Use Each Variant\n",
    "\n",
    "| Variant | Use When | Don't Use When |\n",
    "|---------|----------|----------------|\n",
    "| Gaussian | Features are continuous | Data is discrete |\n",
    "| Multinomial | Working with word counts | Features are binary |\n",
    "| Bernoulli | Features are binary | Need to count occurrences |\n",
    "\n",
    "## 7. Problem-Solving Steps\n",
    "\n",
    "1. Identify data type:\n",
    "   - Continuous → Gaussian\n",
    "   - Count data → Multinomial\n",
    "   - Binary data → Bernoulli\n",
    "\n",
    "2. Check assumptions:\n",
    "   - Feature independence\n",
    "   - Distribution assumptions\n",
    "   - Data quality\n",
    "\n",
    "3. Preprocess data:\n",
    "   - Handle missing values\n",
    "   - Scale if needed (Gaussian)\n",
    "   - Convert to appropriate format\n",
    "\n",
    "4. Choose smoothing:\n",
    "   - Multinomial: vocabulary size\n",
    "   - Bernoulli: number of classes\n",
    "   - Set α value (typically 1)\n",
    "\n",
    "5. Calculate probabilities:\n",
    "   - Use log for numerical stability\n",
    "   - Apply appropriate formula\n",
    "   - Compare results\n",
    "\n",
    "## 8. Common Issues and Solutions\n",
    "\n",
    "| Issue | Solution |\n",
    "|-------|----------|\n",
    "| Zero probabilities | Apply Laplace smoothing |\n",
    "| Numerical underflow | Use log probabilities |\n",
    "| Feature scaling | Standardize for Gaussian |\n",
    "| Class imbalance | Adjust prior probabilities |\n",
    "\n",
    "Remember:\n",
    "- Always scale features for Gaussian NB\n",
    "- Use log probabilities for stability\n",
    "- Consider class balance\n",
    "- Validate independence assumption\n",
    "- Choose appropriate smoothing\n",
    "\n",
    "\n",
    "\n",
    "# Naive Bayes Detailed Problems and Solutions\n",
    "\n",
    "## 1. Gaussian Naive Bayes Problem\n",
    "\n",
    "Problem: Classify students as Pass/Fail based on study hours and sleep hours.\n",
    "\n",
    "Given Data:\n",
    "```\n",
    "Training Data:\n",
    "Pass students:\n",
    "- Study hours: μ = 8, σ = 1\n",
    "- Sleep hours: μ = 6, σ = 0.5\n",
    "\n",
    "Fail students:\n",
    "- Study hours: μ = 4, σ = 1.5\n",
    "- Sleep hours: μ = 8, σ = 1\n",
    "\n",
    "Prior probabilities:\n",
    "P(Pass) = 0.6\n",
    "P(Fail) = 0.4\n",
    "\n",
    "New student:\n",
    "- Study hours = 7\n",
    "- Sleep hours = 7\n",
    "```\n",
    "\n",
    "Solution:\n",
    "```\n",
    "1. Calculate P(features|Pass):\n",
    "   P(study=7|Pass) = 1/(√(2π×1²)) × e^(-(7-8)²/(2×1²))\n",
    "                   = 0.242\n",
    "\n",
    "   P(sleep=7|Pass) = 1/(√(2π×0.5²)) × e^(-(7-6)²/(2×0.5²))\n",
    "                   = 0.107\n",
    "\n",
    "2. Calculate P(features|Fail):\n",
    "   P(study=7|Fail) = 1/(√(2π×1.5²)) × e^(-(7-4)²/(2×1.5²))\n",
    "                   = 0.027\n",
    "\n",
    "   P(sleep=7|Fail) = 1/(√(2π×1²)) × e^(-(7-8)²/(2×1²))\n",
    "                   = 0.242\n",
    "\n",
    "3. Final probabilities:\n",
    "   P(Pass|features) ∝ 0.6 × 0.242 × 0.107 = 0.0155\n",
    "   P(Fail|features) ∝ 0.4 × 0.027 × 0.242 = 0.0026\n",
    "\n",
    "Result: Student likely to Pass (0.0155 > 0.0026)\n",
    "```\n",
    "\n",
    "## 2. Multinomial Naive Bayes Problem\n",
    "\n",
    "Problem: Classify email as Spam/Not Spam based on word frequencies.\n",
    "\n",
    "Given Data:\n",
    "```\n",
    "Training Data:\n",
    "Total emails:\n",
    "- Spam: 100 emails\n",
    "- Not Spam: 200 emails\n",
    "\n",
    "Word frequencies in Spam:\n",
    "- 'money': 50 occurrences\n",
    "- 'win': 40 occurrences\n",
    "- 'free': 60 occurrences\n",
    "\n",
    "Word frequencies in Not Spam:\n",
    "- 'money': 10 occurrences\n",
    "- 'win': 5 occurrences\n",
    "- 'free': 15 occurrences\n",
    "\n",
    "Vocabulary size = 1000 words\n",
    "α = 1 (Laplace smoothing)\n",
    "\n",
    "New email contains: \"free money money\"\n",
    "```\n",
    "\n",
    "Solution:\n",
    "```\n",
    "1. Calculate priors:\n",
    "   P(Spam) = 100/300 = 0.333\n",
    "   P(Not Spam) = 200/300 = 0.667\n",
    "\n",
    "2. Calculate P(word|Spam) with vocabulary smoothing:\n",
    "   P(money|Spam) = (50 + 1)/(150 + 1000) = 0.0444\n",
    "   P(free|Spam) = (60 + 1)/(150 + 1000) = 0.0530\n",
    "\n",
    "3. Calculate P(word|Not Spam):\n",
    "   P(money|Not Spam) = (10 + 1)/(30 + 1000) = 0.0107\n",
    "   P(free|Not Spam) = (15 + 1)/(30 + 1000) = 0.0155\n",
    "\n",
    "4. Final calculation:\n",
    "   P(Spam|email) ∝ 0.333 × 0.0444² × 0.0530 = 3.46 × 10⁻⁵\n",
    "   P(Not Spam|email) ∝ 0.667 × 0.0107² × 0.0155 = 1.19 × 10⁻⁶\n",
    "\n",
    "Result: Classify as Spam (3.46 × 10⁻⁵ > 1.19 × 10⁻⁶)\n",
    "```\n",
    "\n",
    "## 3. Bernoulli Naive Bayes Problem\n",
    "\n",
    "Problem: Classify document based on presence/absence of keywords.\n",
    "\n",
    "Given Data:\n",
    "```\n",
    "Training Data:\n",
    "Documents:\n",
    "- Technical: 150 documents\n",
    "- Non-Technical: 250 documents\n",
    "\n",
    "Word presence in Technical docs:\n",
    "- 'code': 120 documents\n",
    "- 'data': 100 documents\n",
    "- 'algorithm': 90 documents\n",
    "\n",
    "Word presence in Non-Technical docs:\n",
    "- 'code': 20 documents\n",
    "- 'data': 50 documents\n",
    "- 'algorithm': 10 documents\n",
    "\n",
    "α = 1 (Laplace smoothing)\n",
    "Number of classes (k) = 2\n",
    "\n",
    "New document contains: 'code' and 'data' (but no 'algorithm')\n",
    "```\n",
    "\n",
    "Solution:\n",
    "```\n",
    "1. Calculate priors:\n",
    "   P(Technical) = 150/400 = 0.375\n",
    "   P(Non-Technical) = 250/400 = 0.625\n",
    "\n",
    "2. Calculate P(word|Technical) with class smoothing:\n",
    "   P(code|Tech) = (120 + 1)/(150 + 2) = 0.7894\n",
    "   P(data|Tech) = (100 + 1)/(150 + 2) = 0.6645\n",
    "   P(¬algorithm|Tech) = 1 - (90 + 1)/(150 + 2) = 0.4013\n",
    "\n",
    "3. Calculate P(word|Non-Technical):\n",
    "   P(code|Non-Tech) = (20 + 1)/(250 + 2) = 0.0833\n",
    "   P(data|Non-Tech) = (50 + 1)/(250 + 2) = 0.2024\n",
    "   P(¬algorithm|Non-Tech) = 1 - (10 + 1)/(250 + 2) = 0.9562\n",
    "\n",
    "4. Final calculation:\n",
    "   P(Tech|doc) ∝ 0.375 × 0.7894 × 0.6645 × 0.4013 = 0.0791\n",
    "   P(Non-Tech|doc) ∝ 0.625 × 0.0833 × 0.2024 × 0.9562 = 0.0101\n",
    "\n",
    "Result: Classify as Technical (0.0791 > 0.0101)\n",
    "```\n",
    "\n",
    "Key Points to Remember:\n",
    "1. Gaussian NB:\n",
    "   - Use for continuous data\n",
    "   - Calculate mean and standard deviation\n",
    "   - Apply Gaussian formula\n",
    "\n",
    "2. Multinomial NB:\n",
    "   - Use for word frequencies\n",
    "   - Apply vocabulary smoothing\n",
    "   - Count total occurrences\n",
    "\n",
    "3. Bernoulli NB:\n",
    "   - Use for presence/absence\n",
    "   - Apply class smoothing\n",
    "   - Consider both presence and absence\n",
    "\n",
    "Common Steps for All:\n",
    "1. Calculate priors\n",
    "2. Apply appropriate smoothing\n",
    "3. Calculate conditional probabilities\n",
    "4. Multiply probabilities (or add logs)\n",
    "5. Compare final values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Statistical Tests Guide\n",
    "\n",
    "## Common Acronyms and Terms\n",
    "\n",
    "| Acronym/Term | Full Form | Meaning |\n",
    "|-------------|-----------|----------|\n",
    "| ANOVA | Analysis of Variance | Statistical method to analyze differences among group means |\n",
    "| SS | Sum of Squares | Measure of variation from the mean |\n",
    "| SST | Total Sum of Squares | Total variation in the data |\n",
    "| SSB/SSA | Between Groups Sum of Squares | Variation between different groups |\n",
    "| SSW/SSE | Within Groups Sum of Squares/Error | Variation within groups |\n",
    "| df | Degrees of Freedom | Number of values free to vary |\n",
    "| MS | Mean Square | Sum of squares divided by degrees of freedom |\n",
    "| SE | Standard Error | Standard deviation of a sampling distribution |\n",
    "| H₀ | Null Hypothesis | Statement of no effect or difference |\n",
    "| H₁ | Alternative Hypothesis | Statement of effect or difference |\n",
    "| α | Alpha | Significance level (Type I error rate) |\n",
    "| μ | Mu | Population mean |\n",
    "| σ | Sigma | Population standard deviation |\n",
    "| x̄ | x-bar | Sample mean |\n",
    "| s | s | Sample standard deviation |\n",
    "\n",
    "## Test Selection Guide\n",
    "\n",
    "| Test | When to Use | Required Assumptions | Example Scenario |\n",
    "|------|-------------|---------------------|------------------|\n",
    "| One-way ANOVA | Compare means of 3+ groups | Normal distribution, Equal variances | Compare multiple teaching methods |\n",
    "| Two-way ANOVA | Compare effects of 2 factors | Normal distribution, Equal variances | Effect of gender & teaching method |\n",
    "| F-test | Compare variances | Normal distribution | Compare method variabilities |\n",
    "| t-test | Compare means of 2 groups | Normal distribution | Compare control vs treatment |\n",
    "| z-test | Compare with known population | Known population σ, Large sample | Compare to population mean |\n",
    "\n",
    "## 1. One-Way ANOVA\n",
    "\n",
    "### Core Formulas\n",
    "- SST (Total) = Σ(x - x̄)²\n",
    "- SSB (Between) = Σnᵢ(x̄ᵢ - x̄)²\n",
    "- SSW (Within) = SST - SSB\n",
    "- F = (SSB/dfb)/(SSW/dfw)\n",
    "- dfb = k - 1, dfw = N - k\n",
    "\n",
    "### Problem Example\n",
    "```\n",
    "Compare three teaching methods:\n",
    "Method A: 75, 82, 78, 85, 81\n",
    "Method B: 65, 71, 68, 73, 70\n",
    "Method C: 85, 88, 90, 87, 86\n",
    "α = 0.05\n",
    "```\n",
    "\n",
    "### Detailed Solution Steps\n",
    "\n",
    "1. Calculate Group Means:\n",
    "```\n",
    "Method A: x̄A = (75 + 82 + 78 + 85 + 81)/5 = 80.2\n",
    "Method B: x̄B = (65 + 71 + 68 + 73 + 70)/5 = 69.4\n",
    "Method C: x̄C = (85 + 88 + 90 + 87 + 86)/5 = 87.2\n",
    "Grand Mean: x̄ = (80.2 + 69.4 + 87.2)/3 = 78.93\n",
    "```\n",
    "\n",
    "2. Calculate SSB:\n",
    "```\n",
    "SSB = Σnᵢ(x̄ᵢ - x̄)²\n",
    "    = 5(80.2 - 78.93)² + 5(69.4 - 78.93)² + 5(87.2 - 78.93)²\n",
    "    = 5(1.27² + (-9.53)² + 8.27²)\n",
    "    = 5(1.61 + 90.82 + 68.39)\n",
    "    = 804.31\n",
    "```\n",
    "\n",
    "3. Calculate SST:\n",
    "```\n",
    "SST = Σ(x - x̄)²\n",
    "    = (75 - 78.93)² + (82 - 78.93)² + ... + (86 - 78.93)²\n",
    "    = 894.11\n",
    "```\n",
    "\n",
    "4. Calculate SSW:\n",
    "```\n",
    "SSW = SST - SSB = 894.11 - 804.31 = 89.8\n",
    "```\n",
    "\n",
    "5. Calculate Degrees of Freedom:\n",
    "```\n",
    "dfb = k - 1 = 3 - 1 = 2\n",
    "dfw = N - k = 15 - 3 = 12\n",
    "```\n",
    "\n",
    "6. Calculate Mean Squares:\n",
    "```\n",
    "MSB = SSB/dfb = 804.31/2 = 402.16\n",
    "MSW = SSW/dfw = 89.8/12 = 7.48\n",
    "```\n",
    "\n",
    "7. Calculate F-statistic:\n",
    "```\n",
    "F = MSB/MSW = 402.16/7.48 = 53.76\n",
    "F-critical(0.05,2,12) = 3.89\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Since F(53.76) > F-critical(3.89), reject H₀.\n",
    "Teaching methods have significantly different effects on performance.\n",
    "\n",
    "## 2. Two-Way ANOVA\n",
    "\n",
    "### Core Formulas\n",
    "- SST = SSA + SSB + SS(AB) + SSE\n",
    "- FA = MSA/MSE\n",
    "- FB = MSB/MSE\n",
    "- FAB = MSAB/MSE\n",
    "\n",
    "### Problem Example\n",
    "```\n",
    "Effect of Gender and Teaching Method:\n",
    "                Traditional     Online\n",
    "Male:           72,75,71       65,68,63\n",
    "Female:         78,82,80       70,73,71\n",
    "α = 0.05\n",
    "```\n",
    "\n",
    "### Detailed Solution Steps\n",
    "\n",
    "1. Calculate Cell Means:\n",
    "```\n",
    "Male Traditional (MT): x̄MT = (72+75+71)/3 = 72.67\n",
    "Male Online (MO): x̄MO = (65+68+63)/3 = 65.33\n",
    "Female Traditional (FT): x̄FT = (78+82+80)/3 = 80.00\n",
    "Female Online (FO): x̄FO = (70+73+71)/3 = 71.33\n",
    "```\n",
    "\n",
    "2. Calculate Main Effect Means:\n",
    "```\n",
    "Males: x̄M = (72.67+65.33)/2 = 69.00\n",
    "Females: x̄F = (80.00+71.33)/2 = 75.67\n",
    "Traditional: x̄T = (72.67+80.00)/2 = 76.33\n",
    "Online: x̄O = (65.33+71.33)/2 = 68.33\n",
    "Grand Mean: x̄ = (69.00+75.67)/2 = 72.33\n",
    "```\n",
    "\n",
    "3. Calculate Sum of Squares:\n",
    "```\n",
    "SSG (Gender) = 135.37\n",
    "SSM (Method) = 192.67\n",
    "SSI (Interaction) = 4.17\n",
    "SSE (Error) = 313.12\n",
    "SST = 645.33\n",
    "```\n",
    "\n",
    "4. Calculate F-ratios:\n",
    "```\n",
    "F_Gender = MSG/MSE = 135.37/39.14 = 3.46\n",
    "F_Method = MSM/MSE = 192.67/39.14 = 4.92\n",
    "F_Interaction = MSI/MSE = 4.17/39.14 = 0.11\n",
    "F-critical(0.05,1,8) = 5.32\n",
    "```\n",
    "\n",
    "### Conclusions\n",
    "1. Gender Effect (F = 3.46 < 5.32): Not significant\n",
    "2. Method Effect (F = 4.92 < 5.32): Not significant\n",
    "3. Interaction (F = 0.11 < 5.32): No significant interaction\n",
    "\n",
    "## 3. F-Test\n",
    "\n",
    "### Core Formula\n",
    "F = s₁²/s₂² (larger variance/smaller variance)\n",
    "\n",
    "### Problem Example\n",
    "```\n",
    "Compare machine variances:\n",
    "Machine 1: 10.2, 10.4, 10.1, 10.3, 10.2\n",
    "Machine 2: 10.3, 10.1, 10.4, 10.2, 10.3\n",
    "α = 0.05\n",
    "```\n",
    "\n",
    "### Detailed Solution Steps\n",
    "\n",
    "1. Calculate Means:\n",
    "```\n",
    "x̄₁ = (10.2 + 10.4 + 10.1 + 10.3 + 10.2)/5 = 10.24\n",
    "x̄₂ = (10.3 + 10.1 + 10.4 + 10.2 + 10.3)/5 = 10.26\n",
    "```\n",
    "\n",
    "2. Calculate Variances:\n",
    "```\n",
    "s₁² = [(10.2-10.24)² + ... + (10.2-10.24)²]/4 = 0.0130\n",
    "s₂² = [(10.3-10.26)² + ... + (10.3-10.26)²]/4 = 0.0115\n",
    "```\n",
    "\n",
    "3. Calculate F-statistic:\n",
    "```\n",
    "F = 0.0130/0.0115 = 1.13\n",
    "F-critical(0.05,4,4) = 6.39\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Since F(1.13) < F-critical(6.39), cannot reject H₀.\n",
    "No significant difference in variances.\n",
    "\n",
    "## 4. t-Test\n",
    "\n",
    "### Core Formula\n",
    "t = (x̄₁ - x̄₂)/√(s²p(1/n₁ + 1/n₂))\n",
    "where s²p = [(n₁-1)s₁² + (n₂-1)s₂²]/(n₁+n₂-2)\n",
    "\n",
    "### Problem Example\n",
    "```\n",
    "Compare treatments:\n",
    "Control: 68, 72, 70, 71, 65\n",
    "Treatment: 75, 82, 78, 80, 76\n",
    "α = 0.05\n",
    "```\n",
    "\n",
    "### Detailed Solution Steps\n",
    "\n",
    "1. Calculate Means:\n",
    "```\n",
    "Control: x̄₁ = (68 + 72 + 70 + 71 + 65)/5 = 69.2\n",
    "Treatment: x̄₂ = (75 + 82 + 78 + 80 + 76)/5 = 78.2\n",
    "```\n",
    "\n",
    "2. Calculate Sample Variances:\n",
    "```\n",
    "s₁² = [(68-69.2)² + ... + (65-69.2)²]/4 = 7.7\n",
    "s₂² = [(75-78.2)² + ... + (76-78.2)²]/4 = 8.7\n",
    "```\n",
    "\n",
    "3. Calculate Pooled Variance:\n",
    "```\n",
    "s²p = [(4×7.7) + (4×8.7)]/8 = 8.2\n",
    "```\n",
    "\n",
    "4. Calculate t-statistic:\n",
    "```\n",
    "t = (69.2 - 78.2)/√[8.2(2/5)] = -4.97\n",
    "t-critical(0.05,8) = ±2.306\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Since |t| > t-critical, reject H₀.\n",
    "Treatment has significant effect.\n",
    "\n",
    "## 5. z-Test\n",
    "\n",
    "### Core Formula\n",
    "z = (x̄ - μ)/(σ/√n)\n",
    "\n",
    "### Problem Example\n",
    "```\n",
    "Population: μ = 100, σ = 15\n",
    "Sample (n=36): mean = 96\n",
    "α = 0.05\n",
    "```\n",
    "\n",
    "### Detailed Solution Steps\n",
    "\n",
    "1. Calculate Standard Error:\n",
    "```\n",
    "SE = σ/√n = 15/√36 = 2.5\n",
    "```\n",
    "\n",
    "2. Calculate z-statistic:\n",
    "```\n",
    "z = (96 - 100)/2.5 = -1.6\n",
    "z-critical(0.05) = ±1.96\n",
    "```\n",
    "\n",
    "### Conclusion\n",
    "Since |z| < z-critical, cannot reject H₀.\n",
    "Sample mean not significantly different from population mean.\n",
    "\n",
    "## Key Points to Remember\n",
    "\n",
    "1. Test Selection:\n",
    "   - n ≥ 30: Consider z-test\n",
    "   - Compare 2 groups: t-test\n",
    "   - Compare 3+ groups: ANOVA\n",
    "   - Compare variances: F-test\n",
    "\n",
    "2. Critical Values:\n",
    "   - α = 0.05 (common)\n",
    "   - Two-tailed vs One-tailed\n",
    "   - Consider degrees of freedom\n",
    "\n",
    "3. Assumptions:\n",
    "   - Normality\n",
    "   - Equal variances (when applicable)\n",
    "   - Independence\n",
    "   - Random sampling\n",
    "\n",
    "4. Decision Rules:\n",
    "   - If test statistic > critical value: Reject H₀\n",
    "   - If p-value < α: Reject H₀\n",
    "   - Consider practical significance\n",
    "\n",
    "5. Effect Size Measures:\n",
    "   - ANOVA: η² = SSB/SST\n",
    "   - t-test: Cohen's d = (x̄₁ - x̄₂)/s_pooled\n",
    "   - z-test: d = (x̄ - μ)/σ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
